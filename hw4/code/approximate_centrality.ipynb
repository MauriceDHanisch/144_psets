{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vioE_ucBJYd"
      },
      "source": [
        "# Approximate betweenness centrality using neural networks\n",
        "Here we start to approximate the betweennesss centrality using neural networks over a peer-2-peer network Gnutella. Gnutella is a set of datasets consisting of 9 networks ranging from 6,300 to 63,000 nodes. Our goal is to train a neural network on the smallest Gnurella graph and evaluate it on a much larger graph. We will guide you through this step by step.\n",
        "\n",
        "You can find Gnutella datasets at http://snap.stanford.edu/data/index.html. We will use p2p-Gnutella08 for training and p2p-Gnutella04 for testing.\n",
        "\n",
        "Note:\n",
        "1. Copy this notebook to your Google drive in order to execute it.\n",
        "2. Make sure to upload the data files in HW4 to your google drive and to modify their corresponding directories in the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW0BOZ4yfmCv"
      },
      "source": [
        "# Part 1: Training a model on Gnutella 08"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKHtNFA5EadN"
      },
      "source": [
        "## Preprocessing Gnutella08 dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKKzWmx3EYbG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hg8LdYOElHB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jSO_ZIiwUFX"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "\n",
        "# choose an embedding size for Structure2Vec\n",
        "EMBED_SIZE = 64\n",
        "\n",
        "# choose number of dense layers in the neural network\n",
        "NUM_LAYERS = 5\n",
        "\n",
        "# choose number of folds for cross validation\n",
        "NUM_FOLD = 5\n",
        "\n",
        "# choose number of epochs for training\n",
        "NUM_EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIuSDB1JEmpg"
      },
      "outputs": [],
      "source": [
        "# Normalize a list of values\n",
        "# NO NEED TO CHANGE\n",
        "\n",
        "def _normalize_array_by_rank(true_value, nr_nodes):\n",
        "  # true_value is a list of values you want to normalize and nr_nodes is the number of nodes in the list\n",
        "\n",
        "  rank = np.argsort(true_value, kind='mergesort', axis=None) #deg list get's normalised\n",
        "  norm = np.empty([nr_nodes])\n",
        "\n",
        "  for i in range(0, nr_nodes):\n",
        "\n",
        "    norm[rank[i]] = float(i+1) / float(nr_nodes)\n",
        "\n",
        "  max = np.amax(norm)\n",
        "  min = np.amin(norm)\n",
        "  if max > 0.0 and max > min:\n",
        "    for i in range(0, nr_nodes):\n",
        "      norm[i] = 2.0*(float(norm[i] - min) / float(max - min)) - 1.0\n",
        "  else:\n",
        "    print(\"Max value = 0\")\n",
        "\n",
        "  return norm, rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHQb00GNE0Av"
      },
      "outputs": [],
      "source": [
        "#Read in and create NetworkX Graph; G\n",
        "\n",
        "#TO-DO: The path needs to be changed according to your dataset directory in your GOOGLE DRIVE\n",
        "path = '/content/drive/MyDrive/CS144/PS4/P1/p2p-Gnutella08.txt'\n",
        "\n",
        "G = nx.read_edgelist(path, comments='#', delimiter=None, create_using=nx.DiGraph,\n",
        "                  nodetype=None, data=True, edgetype=None, encoding='utf-8')\n",
        "\n",
        "#print(nx.info(G))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMXmUw9HFL3h"
      },
      "outputs": [],
      "source": [
        "# Creating list of Degrees of the nodes in G and normalising them:\n",
        "\n",
        "deg_lst = [val for (node, val) in G.degree()]\n",
        "nr_nodes = G.number_of_nodes()\n",
        "print(\"deg_lst: \\n\", deg_lst)\n",
        "\n",
        "degree_norm, degree_rank = _normalize_array_by_rank(deg_lst, nr_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4a79bglFRux"
      },
      "outputs": [],
      "source": [
        "# Computing Ground-truth values and normalising them:\n",
        "\n",
        "b = [v for v in nx.betweenness_centrality(G).values()]\n",
        "\n",
        "BC_norm_cent, BC_cent_rank = _normalize_array_by_rank(b, nr_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q-1gU8FO9z6"
      },
      "outputs": [],
      "source": [
        "# Define Structure2Vec\n",
        "# NO NEED TO CHANGE\n",
        "\n",
        "def Structure2Vec(G, nr_nodes, degree_norm, num_features=1, embed_size=512, layers=2):\n",
        "\n",
        "  #build feature matrix\n",
        "  def get_degree(i):\n",
        "    return degree_norm[i]\n",
        "\n",
        "  def build_feature_matrix():\n",
        "    n = nr_nodes\n",
        "    feature_matrix = []\n",
        "    for i in range(0, n):\n",
        "      feature_matrix.append(get_degree(i))\n",
        "    return feature_matrix\n",
        "  #Structure2Vec node embedding\n",
        "  A = nx.to_numpy_array(G)\n",
        "\n",
        "  dim = [nr_nodes, num_features]\n",
        "\n",
        "\n",
        "  node_features = tf.cast(build_feature_matrix(), tf.float32)\n",
        "  node_features = tf.reshape(node_features, dim)\n",
        "\n",
        "  initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0,\n",
        "                                                                mode=\"fan_avg\",\n",
        "                                                                distribution=\"uniform\")\n",
        "  #print(initializer)\n",
        "\n",
        "  A = tf.sparse.from_dense(A)\n",
        "  A = tf.cast(A, tf.float32)\n",
        "  w1 = tf.Variable(initializer((num_features, embed_size)), trainable=True,\n",
        "                                  dtype=tf.float32, name=\"w1\")\n",
        "  w2 = tf.Variable(initializer((embed_size, embed_size)), trainable=True,\n",
        "                                  dtype=tf.float32, name=\"w2\")\n",
        "  w3 = tf.Variable(initializer((1,embed_size)), trainable=True, dtype=tf.float32, name=\"w3\")\n",
        "  w4 = tf.Variable(initializer([]), trainable=True, dtype=tf.float32, name=\"w4\")\n",
        "\n",
        "  wx_all = tf.matmul(node_features, w1)  # NxE\n",
        "\n",
        "  #computing X1:\n",
        "  #sparse.reduce_sum: Computes the sum of elements across dimensions of a SparseTensor.\n",
        "  weight_sum_init = tf.sparse.reduce_sum(A, axis=1, keepdims=True, ) #takes adjacency matrix\n",
        "  n_nodes = tf.shape(input=A)[1]\n",
        "\n",
        "  weight_sum = tf.multiply(weight_sum_init, w4)\n",
        "  weight_sum = tf.nn.relu(weight_sum)  # Nx1\n",
        "  weight_sum = tf.matmul(weight_sum, w3)  # NxE\n",
        "\n",
        "  weight_wx = tf.add(wx_all, weight_sum)\n",
        "  current_mu = tf.nn.relu(weight_wx)  # NxE = H^0\n",
        "\n",
        "  for i in range(0, layers):\n",
        "    neighbor_sum = tf.sparse.sparse_dense_matmul(A, current_mu)\n",
        "    neighbor_linear = tf.matmul(neighbor_sum, w2)  # NxE\n",
        "\n",
        "    current_mu = tf.nn.relu(tf.add(neighbor_linear, weight_wx))  # NxE\n",
        "\n",
        "  mu_all = current_mu\n",
        "\n",
        "  return mu_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47p_VK_dRC5f"
      },
      "outputs": [],
      "source": [
        "# Converting the graph structure into vectors\n",
        "\n",
        "mu_all = Structure2Vec(G, nr_nodes, degree_norm, embed_size=EMBED_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J-8ogrRGFoO"
      },
      "source": [
        "## Training a Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwflrFN6F6sx"
      },
      "outputs": [],
      "source": [
        "# Building NN model\n",
        "\n",
        "UNITS = int(EMBED_SIZE/2)\n",
        "def build_model():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.Input(shape=(EMBED_SIZE,)))\n",
        "\n",
        "  # choose the number of layers to construct your network\n",
        "  for _ in range(NUM_LAYERS):\n",
        "    model.add(tf.keras.layers.Dense(UNITS, activation =\"relu\"))\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  model.compile(optimizer='sgd', loss='mse')\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "model = build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSI-9-czGT9j"
      },
      "outputs": [],
      "source": [
        "# Construct training set and groundtruth\n",
        "\n",
        "x_train = mu_all\n",
        "y_train = BC_norm_cent\n",
        "print(tf.shape(x_train))\n",
        "print(tf.shape(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci1l6ODgGVX5"
      },
      "outputs": [],
      "source": [
        "# Computing cross validation\n",
        "# NO NEED TO CHANGE\n",
        "all_scores = []\n",
        "k = NUM_FOLD\n",
        "num_val_samples = len(x_train) // k\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_data = x_train[i*num_val_samples: (i+1) * num_val_samples]\n",
        "  val_targets = y_train[i*num_val_samples: (i+1)*num_val_samples]\n",
        "\n",
        "  partial_train_data = np.concatenate(\n",
        "      [x_train[:i*num_val_samples],\n",
        "      x_train[(i+1)*num_val_samples:]],\n",
        "      axis = 0)\n",
        "  print(tf.shape(partial_train_data))\n",
        "\n",
        "  partial_train_targets = np.concatenate(\n",
        "      [y_train[:i*num_val_samples],\n",
        "      y_train[(i+1)*num_val_samples:]],\n",
        "      axis = 0)\n",
        "\n",
        "  # Training\n",
        "  callbacks =  tf.keras.callbacks.EarlyStopping(\n",
        "      monitor= 'loss', min_delta=0, patience=3, verbose=1,\n",
        "      mode='auto', baseline=None, restore_best_weights=False)\n",
        "\n",
        "  model.fit(partial_train_data, partial_train_targets,\n",
        "            epochs = NUM_EPOCHS, batch_size = 1, callbacks = callbacks, verbose = 1)\n",
        "  print(\"model.metrics_names: \", model.metrics_names)\n",
        "\n",
        "  val_loss = model.evaluate(val_data, val_targets, verbose = 1)\n",
        "\n",
        "  all_scores.append(val_loss)\n",
        "  print(all_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6P1Y4USGzLk"
      },
      "outputs": [],
      "source": [
        "# Computing Kendall on trained set\n",
        "\n",
        "x_new = x_train\n",
        "y_pred = model.predict(x_new)\n",
        "\n",
        "# compute kendalltau using the prediction results and the groundtruth\n",
        "from scipy import stats\n",
        "kendall_tau, p_value = scipy.stats.kendalltau(BC_norm_cent,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTI2M_cyJCqB"
      },
      "outputs": [],
      "source": [
        "# Print your kendalltau score\n",
        "# Make sure your kendalltau score is at least 0.70\n",
        "# PRINT HERE\n",
        "print(kendall_tau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brFQI-QBgYc1"
      },
      "outputs": [],
      "source": [
        "# You could save this model for part 2\n",
        "\n",
        "model.save(\"/content/drive/MyDrive/CS144/PS4/P1/GN08_model_plain.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUWhQjCMfYWd"
      },
      "source": [
        "# Part 2: Evaluating the trained model on Gnutella 04\n",
        "\n",
        "Hints:\n",
        "1. Write down the evaluation using the functions and codes in Part 1\n",
        "2. Compute the groundtruth of betweenness centrality using NetworkX could take around 1 hour. Keep your Colab opened and be patient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bn7__itfjqZ"
      },
      "outputs": [],
      "source": [
        "'''Gnutella 04'''\n",
        "# change the path to your own directory\n",
        "path2 = '/content/drive/MyDrive/CS144/PS4/P1/p2p-Gnutella04.txt'\n",
        "\n",
        "G2 = nx.read_edgelist(path2, comments='#', delimiter=None, create_using=nx.DiGraph,\n",
        "                  nodetype=None, data=True, edgetype=None, encoding='utf-8')\n",
        "\n",
        "#print(nx.info(G2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}